---
title: "Kaggle Competition - Porto Seguro"
author: "Jay Bektasevic"
date: "`r Sys.Date()`"
output: html_document
        
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This is an analysis for the [Porto Seguro's Safe Driver Prediction](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction) competition.
It is sponsored by [Porto Seguro](https://www.portoseguro.com.br/) - a major Brazilian insurance company  who would like to predict which policies are more likely to file a claim within a year. The evaluation metric is normalized Gini coefficient, which measures how well predictions segment the target.

The [data](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/data) comes in the traditional Kaggle form of one training and test file each: `../input/train.csv` & `../input/test.csv`. Each row corresponds to a specific policy holder and the columns describe their features. The target variable is conveniently named **target** here and it indicates whether this policy holder made an insurance claim in the past. 

The training dataset consists of around 595,000 rows and 59 columns, including the dependent variable target. The features in the training set have been given nondescript names, such as ps_ind_01, so any attempt to relate customer information to predictor variables will be difficult.

## Load libraries

We load a range of libraries for general data wrangling and visualisation together with more specialised tools.

```{r, message = FALSE, warning = FALSE, results = "hide"}
packages <- c("data.table",
              "tidyverse",
              "h2o",
              "caret",
              "ggplot2",
              "ROCR",
              "e1071",
              "rpart.plot",
              "rpart")

sapply(packages, require, character.only = TRUE)
```

## Load data

We use *data.table's* **fread()** function to speed up reading in the data, even though in this challenge our files are not very large with about 110 MB for **train** and 165 MB for **test**. Here we are taking into account the fact that missing values in the original data sets are indicated by `-1` or `-1.0` and turn those into "proper" NAs.

```{r warning = FALSE, message = FALSE, results = FALSE}
train_data <- fread("train.csv") 
test_data <- fread("test.csv")
```

for the ease of computetion, let's combine the two datasets.

```{r}
test_data$target <- NA
merged_data <- rbind(train_data, test_data) 
rm(train_data,test_data)

# Feature engineering
merged_data[, amount_nas := rowSums(merged_data == -1, na.rm = T)]
merged_data[, high_nas := ifelse(amount_nas > 4, 1, 0)]
merged_data[, ps_car_13_ps_reg_03 := ps_car_13 * ps_reg_03]
merged_data[, ps_reg_mult := ps_reg_01 * ps_reg_02 * ps_reg_03]
merged_data[, ps_ind_bin_sum := ps_ind_06_bin +  ps_ind_07_bin + ps_ind_08_bin + ps_ind_09_bin + ps_ind_10_bin +
                                ps_ind_11_bin + ps_ind_12_bin + ps_ind_13_bin + ps_ind_16_bin + 
                                ps_ind_17_bin + ps_ind_18_bin]	
```


```{r}

# collect names of all categorical/binary variables
cat_vars <- names(merged_data)[grepl('_cat$', names(merged_data))]

# turn categorical features into factors
merged_data[, (cat_vars) := lapply(.SD, factor), .SDcols = cat_vars]

```
Often when running logistic regression on larger data sets where we've done a lot of factor encoding we may run into issues with linear dependence among the features. If we try to fit a logistic regression where this is the case, we will get a rank-deficient fit. To alleviate this, we can use the findLinearCombos function from the caret package to locate the offending features. Here I run the function and then remove the features as suggested in the remove element of the resulting list.

```{r}
# find any linear combos in features
lin_comb <- findLinearCombos(merged_data[,3:ncol(merged_data)])

# take set difference of feature names and linear combos
diff <- setdiff(seq(1:ncol(merged_data)), lin_comb$remove)

# remove linear combo columns
merged_data1 <- merged_data[ ,62]
```


## Split train dataset

Since, there is no **target** variable in test dataset we'll have to split the training test for validation purposes.

```{r}
#nobs <- nrow(merged_data) 
#merged_data_train <- merged_data[1:10000,]
#train_inx <- sample(nrow(merged_data[!is.na(merged_data)]), 0.8 * nobs)

train_data <- merged_data[!is.na(merged_data$target)]
test_data <- merged_data[is.na(merged_data$target)]
```


# Create a weight column for autoML. Upweight positive class by factor of 2
```{r}

train_data[, weight := ifelse(target == 1, 2, 1)]
```

# Overview

As a first step let's have an overview of the data sets using the **summary()** and **glimpse()** tools.

## Training data

```{r warning = FALSE, message = FALSE}
class(train_data)
glimpse(train_data)
```

We find:

- There are lots of features here. In total, our **training** data has 59 variables, including **id** and **target**. In some of them we already see a number of NAs.

- The [data description](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/data) mentions that the names of the features indicate whether they are binary (**bin**) or categorical (**cat**) variables. Everything else is continuous or ordinal.

- We have already [been told](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/40222) by [Adriano Moala](https://www.kaggle.com/adrianomoala) that the names of the variables indicate certain properties: *Ind" is related to individual or driver, "reg" is related to region, "car" is related to car itself and "calc" is an calculated feature.' Here we will refer to these properties as groups.

- Note, that there is a **ps\_car\_11** as well as a **ps\_car\_11\_cat**. This is the only occasion where the numbering per group is neither consecutive nor unique. Probably a typo in the script that created the variable names.



## Test data:

```{r}
summary(test_data)
```


```{r}
glimpse(test_data)
```

We find: 


## Reformating features

We will turn the categorical features into factors and and will use dummyVars() function form caret package to one hot encode. We created our own function **encode_fun()** to help us with the transformation. Also, the **target** variable will be reformated as a factor.

```{r}
encode_fun <- function(df){
    factor_index <- grep("cat",colnames(df))
    id_index <- 1
    sub_data <- df[ , c(id_index, factor_index)]
    sub_data[,-1] <- apply(sub_data[,-1], 2, as.factor)
    dummy_obj <- dummyVars(id~., data = sub_data)
    dummy_data <- predict(dummy_obj, sub_data)
    cbind(df[, -c(id_index, factor_index)], dummy_data)
    }
```


```{r}
newtrain <- encode_fun(df = data.frame(train_data[,-2]))
newtrain$target <- as.factor(train_data$target)
newtrain <- newtrain[,c(ncol(newtrain), 1:(ncol(newtrain)-1))]
```


# Visualisations 

We start our exploration with overview distribution plots for the various features. In order to make this visualisation more comprehensive, we will create layouts for the specific groups of features. For the sake of readability we divide each group into multiple parts.

These plots will be one of the pillars of our analysis. They might not be particularly exciting in themselves, but whenever we find an interesting effect in one of the variables we can come back here and examine their distribution. It's always an advantage to start with a clear view of the parameter space.


**-> I'll deal with this later on <-**

# Modeling

## Training

We will use *h2o package* wich allows the user to run basic H2O commands using R commands. This will greatly help with the sheer size of the datasets because no actual data is stored in **R** workspace, and no actual work is carried out by R. R rather merely saves the named objects on the server. 

Here we'll initilalize h2o cluster we will also convert our **train** dataset into **as.h2o** format.

we will then build a *Logistic Regression Model* using **h2o.glm()** function and we'll store the results in **log_mod** object.

We've introduced interaction variables into the model.
```{r warning = FALSE, message = FALSE, results = "hide"}

h2o.shutdown() # shutdown existing instance
h2o.init(nthreads = -1)

train.h2o <- as.h2o(newtrain,  destination_frame ="train")


log_mod <- h2o.glm(x = colnames(train.h2o)[-c(1, 50)], 
                   y = "target", 
                   training_frame = train.h2o,
                   family = "binomial",
                   interactions = c('ps_car_13', 'ps_reg_01', 'ps_reg_02', 'ps_reg_03', 'ps_calc_06', 'ps_calc_13',
                                    'ps_calc_04', 'ps_calc_14', 'ps_ind_15', 'ps_calc_07', 
                                     'ps_calc_11',  'ps_calc_09', 'ps_ind_03' ),
                   weights_column = "weight",
                   standardize = TRUE,
                   lambda_search = TRUE)
```

Model details

```{r}
summary(log_mod)
```


# Model Prediction

will also convert our **test** dataset into **as.h2o** format.
```{r warning = FALSE, message = FALSE, results = "hide"}
t_test <- encode_fun(df = as.data.frame(test_data))
test.h2o <- as.h2o(t_test[,-1])
test.h2o <- h2o.cbind(test.h2o, as.h2o(data.frame(weight = rep.int(1, 892816)))) # only if weights are applied
```

We will now use **h2o.predict()** function to make predictions using **test** dataset
```{r warning = FALSE, message = FALSE, results = "hide"}

pred <- h2o.predict(log_mod, test.h2o)
pred <- as.data.frame(pred[, 'p1'])
```

After adding weights and some of interaction variables the submission score remains at .260
Adding interaction variables 'ps_car_13', 'ps_reg_01', 'ps_reg_02', and 'ps_reg_03' bumps the score to .261
## Model Performance 

*ROCR package* will allow us to plot the ROC curve and calculate the AUC (area under the curve) 
which are typical performance measurements for a binary classifier. 

```{r warning = FALSE, message = FALSE, results = "hide"}
pr <- prediction(pred, test_data[,"target"])
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
```


And here is the ROC plot
```{r}
plot(prf, main = "ROC")
abline(0,1, col = "red")
```        

We also calculate the AUC 
```{r}
auc <- performance(pr, measure = "auc")
(auc <- auc@y.values[[1]]) 
```

When submitted this model was given score of 0.6.


#Naive Bayes

since we already have the data loaded in h2o enviroment we can easily run naiveBayes there

```{r}
bayes_mod <- h2o.naiveBayes(x = colnames(train.h2o)[-1], y = "target", 
               training_frame=train.h2o,
               laplace = 3)
```

```{r}
summary(bayes_mod)
```

We will now use **h2o.predict()** function to make predictions using **test** dataset
```{r warning = FALSE, message = FALSE, results = "hide"}
pred <- h2o.predict(bayes_mod, test.h2o, type= "prob")
pred <- as.data.frame(pred[, 'p1'])
```

gini score of 0.189

## Model Performance 

*ROCR package* will allow us to plot the ROC curve and calculate the AUC (area under the curve) 
which are typical performance measurements for a binary classifier. 

```{r warning = FALSE, message = FALSE, results = "hide"}
pr <- prediction(pred, test_data[,"target"])
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
```


And here is the ROC plot
```{r}
plot(prf, main = "ROC")
abline(0,1, col = "red")
```        

We also calculate the AUC 
```{r}
auc <- performance(pr, measure = "auc")
(auc <- auc@y.values[[1]]) 
```


#Random Forest

since we already have the data loaded in h2o enviroment we can easily run naiveBayes there

```{r}
forest_mod <- h2o.randomForest(x = colnames(train.h2o)[-1], y = "target", 
                                training_frame=train.h2o, 
                                ntrees = 100,
                                max_depth = 30,
                                nbins_cats = 1115)
```

```{r}
summary(forest_mod)
```

We will now use **h2o.predict()** func
```{r warning = FALSE, message = FALSE, results = "hide"}
pred <- h2o.predict(forest_mod , test.h2o, type= "prob")
pred <- as.data.frame(pred[, 'p1'])
```

gini score 0.189

#AutoML

The Automatic Machine Learning (AutoML) function automates the supervised machine learning
model training process. The current version of AutoML trains and cross-validates a Random Forest,
an Extremely-Randomized Forest, a random grid of Gradient Boosting Machines (GBMs), a
random grid of Deep Neural Nets, and then trains a Stacked Ensemble using all of the models.

```{r}
aml_model <- h2o.automl(x = colnames(train.h2o)[-c(1, 50)], 
                        y = "target", 
                        training_frame = train.h2o,
                        leaderboard_frame = test.h2o,
                        weights_column = 'weight',
                        max_runtime_secs = 0)
```



#SVM Model

For this feat, we will use **e1071** package. 

Fit SVM model
```{r warning = FALSE, message = FALSE, results = "hide"}
svm_model <- svm(target ~ ., data = train_data, kernel="linear", probability=TRUE)

```


Very broadly speaking with classifiers like this, the predicted value for a binary response variable can be thought of as the probability that that observation belongs to class 1 (in this case your classes are actually labeled 0/1; in other cases you'd need to know which class the function treats as 1 or 0; R often sorts the labels of factors alphabetically and so the last one would be class 1).

we will check the predictions on the test

```{r}
pred_svm <- predict(svm_model, test_data, probability=TRUE )
probabilities <- attr(pred_svm, "probabilities")
head(probabilities)
pr <- prediction(pred_svm, test_data[,"target"])
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
```

And here is the ROC plot
```{r}
plot(prf, main = "ROC Curve")
abline(0,1, col = "red")
```        

We also calculate the AUC 
```{r}
auc <- performance(pr, measure = "auc")
(auc <- auc@y.values[[1]]) 
```
Although SVM model showed potential on smaller dataset, when I attmepted to train it on the entire datase it ran for 13 hours before I terminated it.
At this point I will focus on the other models that showed good results with lot less computationl complexity. 
# Deep Learning (Neural Networks)

```{r}
nnet_mode <- h2o.deeplearning(x = colnames(train.h2o)[-c(1, 50)], 
                 y = "target", 
                 training_frame = train.h2o)

```

```{r}
summary(nnet_mode)
```

We will now use **h2o.predict()** func
```{r warning = FALSE, message = FALSE, results = "hide"}
pred <- h2o.predict(nnet_mode , test.h2o, type= "prob")
pred <- as.data.frame(pred[, 'p1'])
```


# GBM
```{r}
gbm_mod <- h2o.gbm(x = colnames(train.h2o)[-c(1, 50)], 
                 y = "target", 
                 training_frame = train.h2o)
summary(gbm_mod)
```
We will now use **h2o.predict()** func
```{r warning = FALSE, message = FALSE, results = "hide"}
pred <- h2o.predict(gbm_mod , test.h2o, type= "prob")
pred <- as.data.frame(pred[, 'p1'])
```

gbm_model so far is the best first submission .272

# Submission to Kaggle
```{r}
submission_data <- data.frame(id = merged_data[is.na(merged_data$target),]$id, target = as.vector(pred))
colnames(submission_data)[2] <- "target"

fwrite(submission_data, "gbm_model.csv", row.names = FALSE)
```
			 