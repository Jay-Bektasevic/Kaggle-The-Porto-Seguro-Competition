---
title: "Kaggle Competition - Porto Seguro"
author: "Jay Bektasevic"
date: "`r Sys.Date()`"
output: html_document
        
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This is an extensive Exploratory Data Analysis for the [Porto Seguro's Safe Driver Prediction](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction) competition within the R environment of the [tidyverse](http://tidyverse.org/) and [ggplot2](http://ggplot2.tidyverse.org/). We will visualise all the different data features, their relation to the **target** variable, explore multi-parameter interactions, and perform feature engineering.

The aim of this challenge is to predict the probability whether a driver will make an insurance claim, with the purpose of providing a fairer insurance cost on the basis of individual driving habits. It is sponsored by [Porto Seguro](https://www.portoseguro.com.br/) - a major car and home insurance company in Brazil

The [data](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/data) comes in the traditional Kaggle form of one training and test file each: `../input/train.csv` & `../input/test.csv`. Each row corresponds to a specific policy holder and the columns describe their features. The target variable is conveniently named **target** here and it indicates whether this policy holder made an insurance claim in the past. 


# Preparations {.tabset .tabset-fade .tabset-pills}

## Load libraries

We load a range of libraries for general data wrangling and general visualisation together with more specialised tools.

```{r, message = FALSE, warning = FALSE}
packages <- c("data.table",
              "tidyverse",
              "h2o",
              "caret",
              "ggplot2",
              "ROCR" )

sapply(packages, require, character.only = TRUE)
```

## Load data

We use *data.table's* **fread()** function to speed up reading in the data, even though in this challenge our files are not very large with about 110 MB for **train** and 165 MB for **test**. Here we are taking into account the fact that missing values in the original data sets are indicated by `-1` or `-1.0` and turn those into "proper" NAs.

```{r warning = FALSE, message = FALSE, results = FALSE}
seguro_train <- fread("train_test.csv") # <- Replace the file to train.csv

#seguro_test <- fread("test_test.csv") # <- Replace the file to test.csv
```

## Split train dataset

Since, there is no **target** variable in test dataset we'll have to split the training test for validation purposes.

```{r}
nobs <- nrow(seguro_train) 
train_inx <- sample(nrow(seguro_train), 0.8 * nobs)


train_data <- seguro_train[train_inx,]
test_data <- seguro_train[!train_inx]
```

# Overview: File structure and content {.tabset .tabset-fade .tabset-pills}

As a first step let's have an overview of the data sets using the **summary()** and **glimpse()** tools.

## Training data

```{r warning = FALSE, message = FALSE}
class(train_data)
glimpse(train_data)
```

We find:

- There are lots of features here. In total, our **training** data has 59 variables, including **id** and **target**. In some of them we already see a number of NAs.

- The [data description](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/data) mentions that the names of the features indicate whether they are binary (**bin**) or categorical (**cat**) variables. Everything else is continuous or ordinal.

- We have already [been told](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/40222) by [Adriano Moala](https://www.kaggle.com/adrianomoala) that the names of the variables indicate certain properties: *Ind" is related to individual or driver, "reg" is related to region, "car" is related to car itself and "calc" is an calculated feature.' Here we will refer to these properties as groups.

- Note, that there is a **ps\_car\_11** as well as a **ps\_car\_11\_cat**. This is the only occasion where the numbering per group is neither consecutive nor unique. Probably a typo in the script that created the variable names.

- The features are anonymised, because that worked so well in [Mercedes](https://www.kaggle.com/c/mercedes-benz-greener-manufacturing). Just kidding. Mostly ;-)


## Test data:

```{r}
summary(test_data)
```


```{r}
glimpse(test_data)
```

We find: 


## Missing Values Analysis   

This will display count of missing values from each variable.

```{r}
sapply(train_data, function(x) sum(is.na(x)))

sapply(test_data, function(x) sum(is.na(x)))
```

## Reformating features

We will turn the categorical features into factors and and will use dummyVars() function form caret package to one hot encode. We created our own function **encode_fun()** to help us with the transformation. Also, the **target** variable will be reformated as a factor.

```{r}
encode_fun <- function(df){
    factor_index <- grep("cat",colnames(df))
    id_index <- 1
    sub_data <- df[ , c(id_index, factor_index)]
    sub_data[,-1] <- apply(sub_data[,-1], 2, as.factor)
    dummy_obj <- dummyVars(id~., data = sub_data)
    dummy_data <- predict(dummy_obj, sub_data)
    cbind(df[, -c(id_index, factor_index)], dummy_data)
    }
```


```{r}
newtrain <- encode_fun(df = data.frame(train_data))
train_data$target <- as.factor(train_data$target)
```

bind the two datasets, it will help us with the visulations.
```{r}
combine <- bind_rows(train %>% mutate(dset = "train"), 
                     test %>% mutate(dset = "test",
                                     target = NA))
combine <- combine %>% mutate(dset = factor(dset))

```


# Visualisations 

We start our exploration with overview distribution plots for the various features. In order to make this visualisation more comprehensive, we will create layouts for the specific groups of features. For the sake of readability we divide each group into multiple parts.

These plots will be one of the pillars of our analysis. They might not be particularly exciting in themselves, but whenever we find an interesting effect in one of the variables we can come back here and examine their distribution. It's always an advantage to start with a clear view of the parameter space.


**-> I'll deal with this later on <-**

# Modeling

## Training

We will use *h2o package* wich allows the user to run basic H2O commands using R commands. This will greatly help with the sheer size of the datasets because no actual data is stored in **R** workspace, and no actual work is carried out by R. R rather merely saves the named objects on the server. 

Here we'll initilalize h2o cluster we will also convert our **train** dataset into **as.h2o** format.

we will then build a *Logistic Regression Model* using **h2o.glm()** function and we'll store the results in **log_mod** object.
```{r}
h2o.init()
train.h2o <- as.h2o(newtrain,  destination_frame ="train")

log_mod <- h2o.glm(x = colnames(train.h2o)[-1], y = "target", 
               training_frame=train.h2o,
               family = "binomial", 
               standardize = TRUE,
               lambda_search = TRUE)
```

Model details

```{r}
summary(log_mod)
```


# Model Prediction

will also convert our **test** dataset into **as.h2o** format.
```{r}
test_data$target <- as.factor(test_data$target)
t_test <- encode_fun(df = as.data.frame(test_data))
test.h2o <- as.h2o(t_test)
```

We will now use **h2o.predict()** function to make predictions using **test** dataset
```{r}
pred <- h2o.predict(log_mod, train.h2o)
pred <- as.data.frame(pred[, 'p1'])
```


## Model Performance

*ROCR package* will allow us to plot the ROC curve and calculate the AUC (area under the curve) 
which are typical performance measurements for a binary classifier. 

```{r}
    pr <- prediction(pred, train_data[,"target"])
    prf <- performance(pr, measure = "tpr", x.measure = "fpr")
```


And here is the ROC plot
```{r}
    plot(prf, main = "ROC")
    abline(0,1, col = "red")
    
    auc <- performance(pr, measure = "auc")
    auc <- auc@y.values[[1]] 
    auc
```        


# Submission to Kaggle

submission_data <- data.frame(id = test$id, target = as.vector(pred))

fwrite(submission_data, "glm_model.csv", row.names = FALSE)

